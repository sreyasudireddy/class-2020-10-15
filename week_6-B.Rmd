---
title: "Week 6, Day 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)

# Build an urn with 500 red beads, 490 blue beads, and 10 yellow beads. The urn
# should have two variables: ID and color. Mix up the beads in the urn randomly.
# That is, we don't want all the red beads to have ID 1 through 500 and so on.
# Sort the urn by ID.

beads <- c(rep("red", 500), rep("blue", 490), rep("yellow", 10))

urn <- tibble(ID = 1:1000,
              color = sample(beads))

# Using the book, where a very similar example is given, is highly recommended.
# But, this example is trickier than the book because you have to mix up the
# beads before you assign the `ID` values. If you don't tell sample() how many
# you want, it just gives you back everything, reordered. Try `sample(letters)`
# to see. (Note that `letters` is a built in object in R.)
              
```

We are learning about sampling this week. We are taking the theory of Chapter 5 and applying it in a not-too-toyish example. There is a single, true, unknown parameter: the number of red beans in the urn. What is your posterior distribution for that parameter? Once you have that posterior, how can you use it to make forecasts about the future? 

Recall how we (mostly) finished on Tuesday:

```{r prep}
tibble(urn_red = seq(100, 700, by = 100)) %>% 
  mutate(paddle_red = map(urn_red, ~ rbinom(n = 100, size = 25, prob = ./1000))) %>% 
  unnest(paddle_red) %>% 
  sample_n(10)
```

`urn_red` is the assumed number of red beads in the urn, out of 1,000. By assumption, we do not know the true number. Here, we are creating the joint distribution of models-that-might-be-true and results-we-might-see. That is, p(models, data). The above code gets us started in that process, but, obviously, we need to consider many more possible truths. There might be 0 or 1 or 2 or . . . red beads in the urn, after all. `n = 100` means that we are running 100 experiments at a given level of assumed number of red beads in the urn. `unnest()` explodes that list into a long tidy tibble, which will be much easier to work with.



## Scene 1

**Prompt:** As in the book, we will be calculating our posterior distribution of the number of red beads in the urn. Assume that we know that there are 1,000 beads in the urn, all either red, blue or yellow. Create an unnormalized joint distribution of the set of models we are considering and the possible results of our experiment. In this example, we are using a paddle of size 25. (You may want to review the meaning of an unnormalized joint distribution from Chapter 5.) Plot that distribution. It should look very similar to the first plot in Section 6.6.1.

Interpret the meaning of the graphic.




## Scene 2

**Prompt:** Using the joint distribution we calculated in Scene 1 as input, calculate your posterior probability density for the number of red beads in the urn, given that that 10 red were sampled in our paddle of 25.




## Scene 3

**Prompt:** With our posterior probability distribution, we can now forecast outcomes which we have not yet seen. For example, what is the probability of getting more than 3 reds if we sample 20 from the urn, given our posterior? (That is, we are pretending that we don't know the number of red beads in the urn.)



